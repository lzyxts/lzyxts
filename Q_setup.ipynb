{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Pi-k5Ei5-xTI"
      ],
      "mount_file_id": "1wNqDcOo_ELz44_rIW4-PVOgauutx1zIZ",
      "authorship_tag": "ABX9TyMH+5gYzTlKC/APU+KFCA0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lzyxts/lzyxts/blob/main/Q_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "Pi-k5Ei5-xTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DQN/env.py\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "class MarketEnv:\n",
        "    def __init__(self, data, nstock, initial_investment=50000, tran_cost=0.0):\n",
        "        self.data = data  # Trading and Econ data\n",
        "        self.n_step, self.n_stock = self.data.shape[0], nstock  # Declare variables with input data structure\n",
        "        self.stock_prices = self.data[:, :self.n_stock]  # Extract stock prices from the dataset\n",
        "        self.daily_rets = self.data[:, self.n_stock:self.n_stock * 2]  # Extract daily returns from the dataset\n",
        "        self.daily_rets = np.hstack((self.daily_rets, np.zeros((self.n_step, 1))))  # Adding zero return for cash\n",
        "        self.tran_cost = tran_cost  # Transaction cost\n",
        "        self.initial_investment = initial_investment  # Initial investment\n",
        "\n",
        "        # Calculate the size of state dimension, position holdings + market data input\n",
        "        self.state_dim = self.n_stock + self.data.shape[1] + 1\n",
        "\n",
        "        # Initiate other attributes\n",
        "        self.cur_step = None\n",
        "        self.cur_holdings = None\n",
        "        self.stock_price = None\n",
        "        self.daily_ret = None\n",
        "        self.cur_action_idx = None\n",
        "\n",
        "        # Generate a list of possible combinations, exclude [0, 0, 0] here\n",
        "        self.selections = list(map(list, itertools.product([0, 1], repeat=self.n_stock + 1)))[1:]\n",
        "        # Convert to % of the target portfolio allocation\n",
        "        self.action_space = np.array([[val / sum(combo) for val in combo] for combo in self.selections])\n",
        "        # Should equal to 2^(N+1)-1\n",
        "        self.action_space_dim = len(self.action_space)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the environment to the initial state\n",
        "        self.cur_step = 0\n",
        "        self.cur_holdings = np.array([0.0] * self.n_stock + [self.initial_investment])\n",
        "        self.stock_price = self.stock_prices[self.cur_step]\n",
        "        self.daily_ret = self.daily_rets[self.cur_step]\n",
        "        self.portfolio_rets = np.zeros(self.n_step)\n",
        "        self.portfolio_rets[0] = 0.0\n",
        "        self.cur_action_idx = 0\n",
        "        return self.get_state()\n",
        "\n",
        "    # Transaction cost measured the actual dollar amount, 2 basis point would be 0.0002\n",
        "    def step(self, action, verbose=False):\n",
        "        # Get current value before performing the action\n",
        "        prev_eod_val = self.get_val()\n",
        "\n",
        "        if verbose:\n",
        "            print(f'cur_step: {self.cur_step}, position before trade: {self.cur_holdings}, '\n",
        "                  f'port_val before trade: {prev_eod_val}, taking action: {self.action_to_str(action)}')\n",
        "\n",
        "        # Perform the trade and update holdings\n",
        "        self.cur_holdings = self._trade(action, self.cur_holdings, self.tran_cost)\n",
        "\n",
        "        if verbose:\n",
        "            print(f'after trades and relevant transaction cost: {self.cur_holdings} to end the day')\n",
        "\n",
        "        # Update price, i.e. go to the next day\n",
        "        self.cur_step += 1\n",
        "        self.stock_price = self.stock_prices[self.cur_step]\n",
        "        self.daily_ret = self.daily_rets[self.cur_step]\n",
        "        self.cur_holdings *= 1 + self.daily_ret\n",
        "        self.cur_action_idx = action\n",
        "        # Get the new portfolio value after taking the action\n",
        "        cur_val = self.get_val()\n",
        "\n",
        "        # Calculate the portfolio value change as the reward\n",
        "        reward = self.get_reward(cur_val, prev_eod_val)\n",
        "\n",
        "        if verbose:\n",
        "            print(f'EOD pos: {self.cur_holdings}, EOD port val: {cur_val}, reward: {reward}')\n",
        "\n",
        "        # done if we reach the end of the data\n",
        "        done = self.cur_step == (self.n_step - 1)\n",
        "\n",
        "        # Store information about the current situation as needed\n",
        "        info = {'cur_step': self.cur_step, 'cur_val': cur_val}\n",
        "\n",
        "        return self.get_state(), reward, done, info\n",
        "\n",
        "    def get_reward(self, cur_val, prev_eod_val):\n",
        "        # is the change in port val\n",
        "        # Other options also exist, e.g. daily % returns, Sharpe ratio, Sortino ratio, etc.\n",
        "        return (cur_val - prev_eod_val)\n",
        "\n",
        "    def get_state(self):\n",
        "        # Get the current state of the environment\n",
        "        return np.concatenate([self.cur_holdings, self.data[self.cur_step]])\n",
        "\n",
        "    def get_val(self):\n",
        "        # The valuation is simply the sum of all holdings in each asset\n",
        "        return sum(self.cur_holdings)\n",
        "\n",
        "    def action_to_str(self, action):\n",
        "        # Convert action index to action vector (portfolio allocation)\n",
        "        return self.action_space[action]\n",
        "\n",
        "    def _trade(self, action, cur_pos, trans_cost):\n",
        "        # Get the target allocation\n",
        "        action_vec = self.action_space[action]\n",
        "\n",
        "        # assuming we sell everything at the closing price, then buy the target allocation at closing price\n",
        "        # assuming we can purchase fractional shares\n",
        "\n",
        "        tot_val = sum(cur_pos)  # Total portfolio value available to re-allocate\n",
        "        target_allocation = tot_val * action_vec  # Total target value in each asset after rebalance\n",
        "\n",
        "        delta = (target_allocation - cur_pos)[:self.n_stock]  # Determine the assets to sell\n",
        "        tot_trans_cost = sum(delta[delta < 0]) * trans_cost  # Compute the total transaction cost\n",
        "        tot_val += tot_trans_cost  # Compute the total transaction cost\n",
        "\n",
        "        return tot_val * action_vec  # Allocate the portfolio value after taking into account the transaction cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnSx1bm4J98U",
        "outputId": "ff6af1f1-b940-438e-b108-4c7069fe4f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DQN/env.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN Agent"
      ],
      "metadata": {
        "id": "5THhPK-1-zXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DQN/agent.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_inputs, n_action, n_hidden_layers=2, hidden_dim=32):\n",
        "        super(MLP, self).__init__()\n",
        "        M = n_inputs\n",
        "        self.layers = []\n",
        "\n",
        "        # Create hidden layers\n",
        "        for _ in range(n_hidden_layers):\n",
        "            layer = nn.Linear(M, hidden_dim)  # A linear layer, with input size M and output size hidden_dim\n",
        "            M = hidden_dim  # Set the hidden_dim as the input size for the next layer\n",
        "            self.layers.append(layer)\n",
        "            self.layers.append(nn.ReLU())  # A ReLU activation function after each layer\n",
        "\n",
        "        # Final layer, output size = action_space_dim\n",
        "        self.layers.append(nn.Linear(M, n_action))\n",
        "\n",
        "        # Combine all layers into a sequential container\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, X):\n",
        "        return self.layers(X)\n",
        "\n",
        "    # Save and load weights as needed\n",
        "    def save_weights(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load_weights(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "\n",
        "\n",
        "def predict(model, np_states):\n",
        "    # Predict the Q-function associated with each action given the current state\n",
        "    with torch.no_grad():\n",
        "        inputs = torch.from_numpy(np_states.astype(np.float32))\n",
        "        output = model(inputs)\n",
        "        return output.numpy()\n",
        "\n",
        "\n",
        "def train_one_step(model, criterion, optimizer, inputs, targets):\n",
        "    # Type conversion\n",
        "    inputs = torch.from_numpy(inputs.astype(np.float32))\n",
        "    targets = torch.from_numpy(targets.astype(np.float32))\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute the loss term\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backward and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, obs_dim, act_dim, size):\n",
        "        # Buffer initiation, assign space to store current and next observations, actions, rewards, and done flag\n",
        "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros(size, dtype=np.uint8)\n",
        "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.uint8)\n",
        "        self.ptr, self.size, self.max_size = 0, 0, size\n",
        "\n",
        "    def store(self, obs, act, rew, next_obs, done):\n",
        "        # Store a new experience in the buffer\n",
        "        self.obs1_buf[self.ptr] = obs\n",
        "        self.obs2_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size  # Update the pointer to the next location\n",
        "        self.size = min(self.size + 1, self.max_size)  # Update the current buffer size\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        # Sample a batch of experiences from the buffer\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        return dict(s=self.obs1_buf[idxs],\n",
        "                    s2=self.obs2_buf[idxs],\n",
        "                    a=self.acts_buf[idxs],\n",
        "                    r=self.rews_buf[idxs],\n",
        "                    d=self.done_buf[idxs])\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, model):\n",
        "        self.state_size = state_size  # Size of the state space, derived from the dataset fed into MarketEnv\n",
        "        self.action_size = action_size  # Size of the action space, derived from the number of trading assets\n",
        "        self.memory = ReplayBuffer(state_size, action_size, size=500)  # Initiate the Replay Buffer\n",
        "        self.gamma = 0.95  # Discount factor when updating the Q-function\n",
        "        self.epsilon = 1.0  # Epsilon-greedy strategy set up\n",
        "        self.epsilon_min = 0.1  # Minimum exploration rate\n",
        "        self.epsilon_decay = 0.995  # The decay\n",
        "        self.model = model  # The neural network model\n",
        "        self.target_model = model  # Initialize a target model\n",
        "        self.target_model.load_state_dict(model.state_dict())  # Set the same weights as the training model\n",
        "        self.criterion = nn.MSELoss()  # Loss function\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Optimizer\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        # Store a state in the replay buffer\n",
        "        self.memory.store(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state):\n",
        "        # Explore vs Exploit, applying epsilon-greedy strategy\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # Explore, a random action is chosen\n",
        "            return np.random.choice(self.action_size)\n",
        "\n",
        "        # Exploit, predict the Q-functions given the current state\n",
        "        act_values = predict(self.model, state)\n",
        "\n",
        "        # Return the action index that gives the highest Q-value\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size=32):\n",
        "        if self.memory.size < batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample a batch of experience from the replay memory for training\n",
        "        minibatch = self.memory.sample_batch(batch_size)\n",
        "        states = minibatch['s']\n",
        "        actions = minibatch['a']\n",
        "        rewards = minibatch['r']\n",
        "        next_states = minibatch['s2']\n",
        "        done = minibatch['d']\n",
        "\n",
        "        # Predict the target Q-values Q(s',a) using the sample batch and target model\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(predict(self.target_model, next_states), axis=1)\n",
        "        target_full = predict(self.model, states)\n",
        "        target_full[np.arange(batch_size), actions] = target\n",
        "\n",
        "        # Run one training step using the training model\n",
        "        train_one_step(self.model, self.criterion, self.optimizer, states, target_full)\n",
        "\n",
        "        # Update the exploration decay after each training step\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # Update the target model weights to match the model weights\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def load(self, name):\n",
        "        # Load a trained model weight from a specified file\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        # Save model weights for future use\n",
        "        self.model.save_weights(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5pqmKe4DSw4",
        "outputId": "8e28d6a0-5c44-4fb4-c8b4-3b3e604cd2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DQN/agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F8vtme-mbalt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}